{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import pos_tag, word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('gutenberg')\n",
    "# import re\n",
    "# import string\n",
    "# from itertools import combinations\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import operator\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-24 17:02:31,528 loading file /Users/matijagercer/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n",
      "2022-05-24 17:02:35,741 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Use flair named entity recognition\n",
    "tagger = SequenceTagger.load('ner')     # ner, ner-ontonotes, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ')\\\n",
    "            .replace(\"’\", \"'\").replace(\"\\\"\", \"\").replace(\"”\", \"\").replace(\"“\", \"\")\n",
    "    return text\n",
    "\n",
    "def flair_NER(book, eval = False):\n",
    "    \"\"\"\n",
    "    flair_NER vrne seznam, v katerem so shranjene prepoznane identitete glede na posamezni stavek\n",
    "    :param book: str\n",
    "    :return: entity_dict (seznam slovarjev kot npr. [name, tag, start_pos, stop_pos, line_num. token_num])\n",
    "    \"\"\"\n",
    "    \n",
    "    # EVAL ---\n",
    "    if eval == True:\n",
    "        sentences, sentences_tag = conll_read()\n",
    "        #print(len(sentences), \"--------\")\n",
    "    else:\n",
    "        sentences = sent_tokenize(book)\n",
    "    sentences_NER = []\n",
    "    sentences_NER_results = []\n",
    "    # EVAL ---\n",
    "\n",
    "    \n",
    "    \n",
    "    # 01 NER model\n",
    "    entity_dict = []\n",
    "    for line_num, line in enumerate(tqdm(sentences)):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # EVAL ---\n",
    "        words_list = word_tokenize(line)\n",
    "        tags_list = [\"O\"] * len(words_list)\n",
    "        sentences_NER.append(words_list)\n",
    "        # EVAL ---\n",
    "        \n",
    "        \n",
    "\n",
    "        # 01a Predict:\n",
    "        sentence = Sentence(line)\n",
    "        tagger.predict(sentence)\n",
    "\n",
    "        # 01b Vmesni izpis (brez lokacije):\n",
    "        #print(sentence.to_tagged_string())\n",
    "        #print(sentence.get_spans('ner'))\n",
    "\n",
    "        # 01c Ekstrakcija podatka iz stavka:\n",
    "        for entity in sentence.get_spans('ner'):        # veliko različnih flair - nerov\n",
    "            name = entity.text\n",
    "\n",
    "            # str location\n",
    "            # start_pos = entity.start_position    # št. str\n",
    "            # stop_pos = entity.end_position      # št. str\n",
    "\n",
    "            # token location\n",
    "            tmp_flag = True\n",
    "            for token in entity:\n",
    "                if tmp_flag:\n",
    "                    start_pos = token.idx -1\n",
    "                    stop_pos = token.idx\n",
    "                else:\n",
    "                    stop_pos = token.idx\n",
    "                tmp_flag = False\n",
    "\n",
    "            tag = entity.get_label(\"ner\").value             # tag = entity.tag\n",
    "            conf_score = entity.get_label(\"ner\").score      # conf_score = entity.score\n",
    "\n",
    "            info_dict = {}\n",
    "            info_dict[\"name\"] = name\n",
    "            info_dict[\"tag\"] = tag\n",
    "            info_dict[\"start_pos\"] = start_pos\n",
    "            info_dict[\"stop_pos\"] = stop_pos\n",
    "            info_dict[\"line_num\"] = line_num\n",
    "\n",
    "            entity_dict.append(info_dict)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # EVAL: ---\n",
    "            names = name.split(\" \")\n",
    "            ff_flag = False\n",
    "            if tag == \"PER\":\n",
    "                for name in names:\n",
    "                    try:\n",
    "                        if ff_flag == False:\n",
    "                            idx = words_list.index(name)\n",
    "                            tags_list[idx] = \"B-PER\"\n",
    "                            ff_flag = True\n",
    "                        else:\n",
    "                            idx = words_list.index(name)\n",
    "                            tags_list[idx] = \"I-PER\"\n",
    "                    except:\n",
    "                        print(\"Not found: \", name)\n",
    "            # EVAL ---\n",
    "            \n",
    "        sentences_NER_results.append(tags_list)\n",
    "            \n",
    "    return entity_dict, sentences_NER, sentences_NER_results\n",
    "\n",
    "\n",
    "\n",
    "def get_names_from_NER(entity_dict):\n",
    "    \"\"\"\n",
    "    get_names_from_NER sprejme entity_dict in vrne urejen seznam terk (\"ime\", št_zaznano)\n",
    "    :param entity_dict: dict (seznam dictov)\n",
    "    :return: unique_names: list\n",
    "    \"\"\"\n",
    "    unique_names = {}\n",
    "\n",
    "    for entity in entity_dict:\n",
    "        if entity[\"tag\"] == \"PER\":\n",
    "            if entity[\"name\"] not in unique_names:\n",
    "                unique_names[entity[\"name\"]] = 1\n",
    "            else:\n",
    "                unique_names[entity[\"name\"]] += 1\n",
    "    unique_names = sorted(unique_names.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return unique_names\n",
    "\n",
    "\n",
    "def conll_read():\n",
    "    with open(\"conll2003/valid.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    sentences = []\n",
    "    sentences_tag = []\n",
    "    tmp = \"\"\n",
    "    tmp_tag = \"\"\n",
    "    tags = []\n",
    "    for i in range(len(lines)):\n",
    "\n",
    "        line = lines[i]\n",
    "        line = line.split(\" \")\n",
    "        text_word = line[0]\n",
    "\n",
    "        if text_word[0] != \"-\":\n",
    "            if text_word == \"\\n\":\n",
    "                # print(tmp)\n",
    "                # print(tmp_tag)\n",
    "                # print(\"-------\")\n",
    "                sentences.append(tmp)\n",
    "                tmp = \"\"\n",
    "                #sentences_tag.append(tmp_tag)\n",
    "                #tmp_tag = \"\"\n",
    "                sentences_tag.append(tags)\n",
    "                tags = []\n",
    "\n",
    "            elif text_word == \"(\" or text_word == \")\" or text_word == '\"' or text_word == ':':\n",
    "                pass\n",
    "            else:\n",
    "                if text_word[0] != \".\" and text_word[0] != \",\" and text_word[0] != \":\" and text_word[0] != \";\"\\\n",
    "                        and text_word[0] != \"!\" and text_word[0] != \"?\" and text_word[0] != \")\" and text_word[0] != \"'\":   #or text_word[0] != \",\" or text_word[0] != \"(\" or text_word[0] != \")\":\n",
    "                    tmp = tmp + \" \" + text_word\n",
    "                    line[3] = line[3].replace(\"\\n\", \"\")\n",
    "\n",
    "                    if line[3] == \"B-PER\" or line[3] == \"I-PER\":\n",
    "                        #line[3] = \"B-PER\"\n",
    "                        #tmp_tag = tmp_tag + \" \" + line[3]\n",
    "                        tags.append(line[3])\n",
    "                    else:\n",
    "                        line[3] = \"O\"\n",
    "                        #tmp_tag = tmp_tag + \" \" + line[3]\n",
    "                        tags.append(line[3])\n",
    "                else:\n",
    "                    tmp = tmp +  text_word\n",
    "                    line[3] = \"O\"\n",
    "                    tags.append(line[3])\n",
    "    return sentences, sentences_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 284/284 [02:38<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bran 46\n",
      "Robb 29\n",
      "Jon 28\n",
      "Jory 9\n",
      "Greyjoy 7\n",
      "Theon Greyjoy 6\n",
      "Father 5\n",
      "Hullen 5\n",
      "Jon Snow 3\n",
      "Old Nan 2\n",
      "Lord Stark 2\n",
      "Jory Cassel 2\n",
      "House Stark 2\n",
      "Stark 2\n",
      "Mance Rayder 1\n",
      "Eddard Stark 1\n",
      "Robert of the House Baratheon 1\n",
      "Eddard 1\n",
      "Lord of Winterfell 1\n",
      "Theon 1\n",
      "Robert 1\n",
      "Starks 1\n",
      "Gods 1\n",
      "Harwin 1\n",
      "Ser Rodrik 1\n",
      "Rickon 1\n",
      "Snow 1\n",
      "Desmond 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "book = read_text('../../data/books/ASongOfIceAndFire/AGOT/chapters/1_Bran_1.txt')\n",
    "entity_dict, sentences_NER, sentences_NER_results = flair_NER(book, eval= False)\n",
    "unique_names = get_names_from_NER(entity_dict)\n",
    "\n",
    "for (name, num) in unique_names:\n",
    "    if True: #num > 1:\n",
    "        print(name, num)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evalvacija na datasetu Conll2003 (osredotočena na imena)\n",
    "\n",
    "def eval_wrapper():\n",
    "    entity_dict, sentences_NER, sentences_NER_results = flair_NER(\"\", eval= True)\n",
    "    unique_names = get_names_from_NER(entity_dict)\n",
    "\n",
    "    # Kontrola podatkov\n",
    "    sentences_org, sentences_org_tag = conll_read()\n",
    "    # for x in range(len(sentences_NER)):\n",
    "    #     print(\"-----------\", x)\n",
    "    #     #print(len(sentences_org_tag[x]))\n",
    "    #     print(len(sentences_NER[x]))\n",
    "    #     print(len(sentences_NER_results[x]))\n",
    "\n",
    "    #     #print(sentences_org[x])    # 22 26\n",
    "    #     print(sentences_NER[x])\n",
    "    #     print(sentences_org_tag[x])\n",
    "    #     print(sentences_NER_results[x])\n",
    "    \n",
    "    true = sentences_org_tag\n",
    "    pred = sentences_NER_results\n",
    "    evaluator = Evaluator(true, pred, tags=['PER'], loader=\"list\")\n",
    "\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    "    print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evalvacija za I. poglavje GoT\n",
    "\n",
    "from chapter__1annotated import chapter1_lst\n",
    "\n",
    "# Kontrola:\n",
    "# print(len(sentences_NER_results))\n",
    "# print(len(chapter1_lst))\n",
    "\n",
    "# for x in range(len(sentences_NER)):\n",
    "#     print(\"-----------\", x)\n",
    "#     print(len(sentences_NER[x]))\n",
    "#     print(len(sentences_NER_results[x]))\n",
    "#     print(len(chapter1_lst[x]))\n",
    "    \n",
    "\n",
    "#     print(sentences_NER[x])\n",
    "#     print(sentences_NER_results[x])\n",
    "#     print(chapter1_lst[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': {'correct': 156, 'incorrect': 0, 'partial': 0, 'missed': 8, 'spurious': 7, 'possible': 164, 'actual': 163, 'precision': 0.9570552147239264, 'recall': 0.9512195121951219, 'f1': 0.9541284403669725}, 'partial': {'correct': 155, 'incorrect': 0, 'partial': 1, 'missed': 8, 'spurious': 7, 'possible': 164, 'actual': 163, 'precision': 0.9539877300613497, 'recall': 0.948170731707317, 'f1': 0.9510703363914372}, 'strict': {'correct': 155, 'incorrect': 1, 'partial': 0, 'missed': 8, 'spurious': 7, 'possible': 164, 'actual': 163, 'precision': 0.950920245398773, 'recall': 0.9451219512195121, 'f1': 0.948012232415902}, 'exact': {'correct': 155, 'incorrect': 1, 'partial': 0, 'missed': 8, 'spurious': 7, 'possible': 164, 'actual': 163, 'precision': 0.950920245398773, 'recall': 0.9451219512195121, 'f1': 0.948012232415902}}\n"
     ]
    }
   ],
   "source": [
    "true = chapter1_lst\n",
    "pred = sentences_NER_results\n",
    "evaluator = Evaluator(true, pred, tags=['PER'], loader=\"list\")\n",
    "\n",
    "results, results_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
