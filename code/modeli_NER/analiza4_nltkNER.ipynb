{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import operator\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ')\\\n",
    "            .replace(\"’\", \"'\").replace(\"\\\"\", \"\").replace(\"”\", \"\").replace(\"“\", \"\")\n",
    "    return text\n",
    "\n",
    "def nltk_NER(book, eval = False):\n",
    "    \"\"\"\n",
    "    nlkt_NER vrne seznam, v katerem so shranjene prepoznane identitete glede na posamezni stavek\n",
    "    :param book: str\n",
    "    :return: entity_dict (seznam slovarjev kot npr. [name, tag, start_pos, stop_pos, line_num. token_num])\n",
    "    \"\"\"\n",
    "    \n",
    "    # EVAL ---\n",
    "    if eval == True:\n",
    "        sentences, sentences_tag = conll_read()\n",
    "        #print(len(sentences), \"--------\")\n",
    "    else:\n",
    "        sentences = sent_tokenize(book)\n",
    "    sentences_NER = []\n",
    "    sentences_NER_results = []\n",
    "    # EVAL ---\n",
    "    \n",
    "    entity_dict = []\n",
    "\n",
    "    for line_num, line in enumerate(sentences):\n",
    "        \n",
    "        # EVAL ---\n",
    "        words_list = word_tokenize(line)\n",
    "        tags_list = [\"O\"] * len(words_list)\n",
    "        sentences_NER.append(words_list)\n",
    "        # EVAL ---\n",
    "\n",
    "        words = nltk.word_tokenize(line)\n",
    "        pos_tag = nltk.pos_tag(words)\n",
    "        ne_chunk = nltk.ne_chunk(pos_tag, binary=False)\n",
    "\n",
    "        token_num = 0\n",
    "        for chunk in ne_chunk:\n",
    "            if hasattr(chunk,'label'):\n",
    "                start_pos = token_num\n",
    "                stop_pos = token_num + len(chunk)\n",
    "                tag = chunk.label()\n",
    "                name = ' '.join(c[0] for c in chunk)\n",
    "                token_num += len(chunk)\n",
    "\n",
    "                info_dict = {}\n",
    "                info_dict[\"name\"] = name\n",
    "                info_dict[\"tag\"] = tag\n",
    "                info_dict[\"start_pos\"] = start_pos\n",
    "                info_dict[\"stop_pos\"] = stop_pos\n",
    "                info_dict[\"line_num\"] = line_num\n",
    "                entity_dict.append(info_dict)\n",
    "                \n",
    "                #print(name, tag)\n",
    "                # EVAL: ---\n",
    "                #names = word.text.replace(\"'s\", \"\")\n",
    "                names = name.split(\" \")\n",
    "                ff_flag = False\n",
    "                if tag == \"PERSON\":\n",
    "                    for name in names:\n",
    "                        try:\n",
    "                            if ff_flag == False:\n",
    "                                idx = words_list.index(name)\n",
    "                                tags_list[idx] = \"B-PER\"\n",
    "                                ff_flag = True\n",
    "                            else:\n",
    "                                idx = words_list.index(name)\n",
    "                                tags_list[idx] = \"I-PER\"\n",
    "                        except:\n",
    "                            print(\"Not found: \", name)\n",
    "                # EVAL ---\n",
    "            \n",
    "                \n",
    "            else:\n",
    "                token_num += 1\n",
    "        sentences_NER_results.append(tags_list) \n",
    "\n",
    "\n",
    "    return entity_dict, sentences_NER, sentences_NER_results\n",
    "\n",
    "\n",
    "def get_names_from_NER(entity_dict):\n",
    "    \"\"\"\n",
    "    get_names_from_NER sprejme entity_dict in vrne urejen seznam terk (\"ime\", št_zaznano)\n",
    "    :param entity_dict: dict (seznam dictov)\n",
    "    :return: unique_names: list\n",
    "    \"\"\"\n",
    "    unique_names = {}\n",
    "\n",
    "    for entity in entity_dict:\n",
    "        if entity[\"tag\"] == \"PERSON\":\n",
    "            if entity[\"name\"] not in unique_names:\n",
    "                unique_names[entity[\"name\"]] = 1\n",
    "            else:\n",
    "                unique_names[entity[\"name\"]] += 1\n",
    "    unique_names = sorted(unique_names.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return unique_names\n",
    "\n",
    "\n",
    "def conll_read():\n",
    "    with open(\"conll2003/valid.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    sentences = []\n",
    "    sentences_tag = []\n",
    "    tmp = \"\"\n",
    "    tmp_tag = \"\"\n",
    "    tags = []\n",
    "    for i in range(len(lines)):\n",
    "\n",
    "        line = lines[i]\n",
    "        line = line.split(\" \")\n",
    "        text_word = line[0]\n",
    "\n",
    "        if text_word[0] != \"-\":\n",
    "            if text_word == \"\\n\":\n",
    "                # print(tmp)\n",
    "                # print(tmp_tag)\n",
    "                # print(\"-------\")\n",
    "                sentences.append(tmp)\n",
    "                tmp = \"\"\n",
    "                #sentences_tag.append(tmp_tag)\n",
    "                #tmp_tag = \"\"\n",
    "                sentences_tag.append(tags)\n",
    "                tags = []\n",
    "\n",
    "            elif text_word == \"(\" or text_word == \")\" or text_word == '\"' or text_word == ':':\n",
    "                pass\n",
    "            else:\n",
    "                if text_word[0] != \".\" and text_word[0] != \",\" and text_word[0] != \":\" and text_word[0] != \";\"\\\n",
    "                        and text_word[0] != \"!\" and text_word[0] != \"?\" and text_word[0] != \")\" and text_word[0] != \"'\":   #or text_word[0] != \",\" or text_word[0] != \"(\" or text_word[0] != \")\":\n",
    "                    tmp = tmp + \" \" + text_word\n",
    "                    line[3] = line[3].replace(\"\\n\", \"\")\n",
    "\n",
    "                    if line[3] == \"B-PER\" or line[3] == \"I-PER\":\n",
    "                        #line[3] = \"B-PER\"\n",
    "                        #tmp_tag = tmp_tag + \" \" + line[3]\n",
    "                        tags.append(line[3])\n",
    "                    else:\n",
    "                        line[3] = \"O\"\n",
    "                        #tmp_tag = tmp_tag + \" \" + line[3]\n",
    "                        tags.append(line[3])\n",
    "                else:\n",
    "                    tmp = tmp +  text_word\n",
    "                    line[3] = \"O\"\n",
    "                    tags.append(line[3])\n",
    "    return sentences, sentences_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bran 42\n",
      "Jon 29\n",
      "Robb 27\n",
      "Father 8\n",
      "Jory 7\n",
      "Greyjoy 5\n",
      "Hullen 4\n",
      "Snow 3\n",
      "Lord 2\n",
      "Robert 2\n",
      "Jon Snow 2\n",
      "Theon Greyjoy 2\n",
      "Mance Rayder 1\n",
      "Old Nan 1\n",
      "Eddard Stark 1\n",
      "Jory Cassel 1\n",
      "Blood 1\n",
      "Stark 1\n",
      "Old 1\n",
      "Nan 1\n",
      "Watch 1\n",
      "Harwin 1\n",
      "Ser 1\n",
      "Rodrik 1\n",
      "Rickon 1\n",
      "Desmond 1\n"
     ]
    }
   ],
   "source": [
    "book = read_text('../../data/books/ASongOfIceAndFire/AGOT/chapters/1_Bran_1.txt')\n",
    "entity_dict, sentences_NER, sentences_NER_results = nltk_NER(book, False)\n",
    "unique_names = get_names_from_NER(entity_dict)\n",
    "\n",
    "for (name, num) in unique_names:\n",
    "    if True: #num > 1:\n",
    "        print(name, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evalvacija na datasetu Conll2003 (osredotočena na imena)\n",
    "\n",
    "def eval_wrapper():\n",
    "    entity_dict, sentences_NER, sentences_NER_results = nltk_NER(\"\", eval= True)\n",
    "    unique_names = get_names_from_NER(entity_dict)\n",
    "\n",
    "    # Kontrola podatkov\n",
    "    sentences_org, sentences_org_tag = conll_read()\n",
    "    # for x in range(len(sentences_NER)):\n",
    "    #     print(\"-----------\", x)\n",
    "    #     #print(len(sentences_org_tag[x]))\n",
    "    #     print(len(sentences_NER[x]))\n",
    "    #     print(len(sentences_NER_results[x]))\n",
    "\n",
    "    #     #print(sentences_org[x])    # 22 26\n",
    "    #     print(sentences_NER[x])\n",
    "    #     print(sentences_org_tag[x])\n",
    "    #     print(sentences_NER_results[x])\n",
    "    \n",
    "    true = sentences_org_tag\n",
    "    pred = sentences_NER_results\n",
    "    evaluator = Evaluator(true, pred, tags=['PER'], loader=\"list\")\n",
    "\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': {'correct': 137, 'incorrect': 0, 'partial': 0, 'missed': 27, 'spurious': 7, 'possible': 164, 'actual': 144, 'precision': 0.9513888888888888, 'recall': 0.8353658536585366, 'f1': 0.8896103896103895}, 'partial': {'correct': 128, 'incorrect': 0, 'partial': 9, 'missed': 27, 'spurious': 7, 'possible': 164, 'actual': 144, 'precision': 0.9201388888888888, 'recall': 0.8079268292682927, 'f1': 0.8603896103896104}, 'strict': {'correct': 128, 'incorrect': 9, 'partial': 0, 'missed': 27, 'spurious': 7, 'possible': 164, 'actual': 144, 'precision': 0.8888888888888888, 'recall': 0.7804878048780488, 'f1': 0.8311688311688312}, 'exact': {'correct': 128, 'incorrect': 9, 'partial': 0, 'missed': 27, 'spurious': 7, 'possible': 164, 'actual': 144, 'precision': 0.8888888888888888, 'recall': 0.7804878048780488, 'f1': 0.8311688311688312}}\n"
     ]
    }
   ],
   "source": [
    "## Evalvacija za I. poglavje GoT\n",
    "\n",
    "from chapter__1annotated import chapter1_lst\n",
    "\n",
    "# Kontrola:\n",
    "# print(len(sentences_NER_results))\n",
    "# print(len(chapter1_lst))\n",
    "\n",
    "# for x in range(len(sentences_NER)):\n",
    "#     print(\"-----------\", x)\n",
    "#     print(len(sentences_NER[x]))\n",
    "#     print(len(sentences_NER_results[x]))\n",
    "#     print(len(chapter1_lst[x]))\n",
    "    \n",
    "\n",
    "#     print(sentences_NER[x])\n",
    "#     print(sentences_NER_results[x])\n",
    "#     print(chapter1_lst[x])\n",
    "\n",
    "true = chapter1_lst\n",
    "pred = sentences_NER_results\n",
    "evaluator = Evaluator(true, pred, tags=['PER'], loader=\"list\")\n",
    "\n",
    "results, results_by_tag = evaluator.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
