{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/matijagercer/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "import operator\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Prime/NNP\n",
      "  Minister/NNP\n",
      "  (NE Jacinda/NNP Ardern/NNP)\n",
      "  has/VBZ\n",
      "  claimed/VBN\n",
      "  that/IN\n",
      "  (NE New/NNP Zealand/NNP)\n",
      "  had/VBD\n",
      "  won/VBN\n",
      "  a/DT\n",
      "  big/JJ\n",
      "  battle/NN\n",
      "  over/IN\n",
      "  the/DT\n",
      "  spread/NN\n",
      "  of/IN\n",
      "  coronavirus/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent= '''Prime Minister Jacinda Ardern has claimed that New Zealand had won a big\n",
    "battle over the spread of coronavirus.'''\n",
    "\n",
    "#sentences = sent_tokenize(sent)\n",
    "\n",
    "words= word_tokenize(sent)\n",
    "tags=pos_tag(words)\n",
    "ne = nltk.ne_chunk(tags,binary=True)\n",
    "print(ne)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Prime', 'NNP', 'O'),\n ('Minister', 'NNP', 'O'),\n ('Jacinda', 'NNP', 'B-NE'),\n ('Ardern', 'NNP', 'I-NE'),\n ('has', 'VBZ', 'O'),\n ('claimed', 'VBN', 'O'),\n ('that', 'IN', 'O'),\n ('New', 'NNP', 'B-NE'),\n ('Zealand', 'NNP', 'I-NE'),\n ('had', 'VBD', 'O'),\n ('won', 'VBN', 'O'),\n ('a', 'DT', 'O'),\n ('big', 'JJ', 'O'),\n ('battle', 'NN', 'O'),\n ('over', 'IN', 'O'),\n ('the', 'DT', 'O'),\n ('spread', 'NN', 'O'),\n ('of', 'IN', 'O'),\n ('coronavirus', 'NN', 'O'),\n ('.', '.', 'O')]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk import tree2conlltags\n",
    "iob = tree2conlltags(ne)\n",
    "iob\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\r', ' ').replace('\\n', ' ')\\\n",
    "            .replace(\"’\", \"'\").replace(\"\\\"\", \"\").replace(\"”\", \"\").replace(\"“\", \"\")\n",
    "    return text\n",
    "\n",
    "def nltk_NER(book):\n",
    "    \"\"\"\n",
    "    nlkt_NER vrne seznam, v katerem so shranjene prepoznane identitete glede na posamezni stavek\n",
    "    :param book: str\n",
    "    :return: entity_dict (seznam slovarjev kot npr. [name, tag, start_pos, stop_pos, line_num. token_num])\n",
    "    \"\"\"\n",
    "\n",
    "    # 00 Pretvori knjigo v stavke:\n",
    "    sentences = sent_tokenize(book)\n",
    "    entity_dict = []\n",
    "\n",
    "    for line_num, line in enumerate(sentences):\n",
    "\n",
    "        words = nltk.word_tokenize(line)\n",
    "        pos_tag = nltk.pos_tag(words)\n",
    "        ne_chunk = nltk.ne_chunk(pos_tag, binary=False)\n",
    "\n",
    "        token_num = 0\n",
    "        for chunk in ne_chunk:\n",
    "            if hasattr(chunk,'label'):\n",
    "                start_pos = token_num\n",
    "                stop_pos = token_num + len(chunk)\n",
    "                tag = chunk.label()\n",
    "                name = ' '.join(c[0] for c in chunk)\n",
    "                token_num += len(chunk)\n",
    "\n",
    "                info_dict = {}\n",
    "                info_dict[\"name\"] = name\n",
    "                info_dict[\"tag\"] = tag\n",
    "                info_dict[\"start_pos\"] = start_pos\n",
    "                info_dict[\"stop_pos\"] = stop_pos\n",
    "                info_dict[\"line_num\"] = line_num\n",
    "                entity_dict.append(info_dict)\n",
    "            else:\n",
    "                token_num += 1\n",
    "\n",
    "\n",
    "\n",
    "    return entity_dict\n",
    "\n",
    "\n",
    "def get_names_from_NER(entity_dict):\n",
    "    \"\"\"\n",
    "    get_names_from_NER sprejme entity_dict in vrne urejen seznam terk (\"ime\", št_zaznano)\n",
    "    :param entity_dict: dict (seznam dictov)\n",
    "    :return: unique_names: list\n",
    "    \"\"\"\n",
    "    unique_names = {}\n",
    "\n",
    "    for entity in entity_dict:\n",
    "        if entity[\"tag\"] == \"PERSON\":\n",
    "            if entity[\"name\"] not in unique_names:\n",
    "                unique_names[entity[\"name\"]] = 1\n",
    "            else:\n",
    "                unique_names[entity[\"name\"]] += 1\n",
    "    unique_names = sorted(unique_names.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return unique_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bran 42\n",
      "Jon 29\n",
      "Robb 27\n",
      "Father 8\n",
      "Jory 7\n",
      "Greyjoy 5\n",
      "Hullen 4\n",
      "Snow 3\n",
      "Lord 2\n",
      "Robert 2\n",
      "Jon Snow 2\n",
      "Theon Greyjoy 2\n",
      "Mance Rayder 1\n",
      "Old Nan 1\n",
      "Eddard Stark 1\n",
      "Jory Cassel 1\n",
      "Blood 1\n",
      "Stark 1\n",
      "Old 1\n",
      "Nan 1\n",
      "Watch 1\n",
      "Harwin 1\n",
      "Ser 1\n",
      "Rodrik 1\n",
      "Rickon 1\n",
      "Desmond 1\n"
     ]
    }
   ],
   "source": [
    "book = read_text('../../data/books/ASongOfIceAndFire/AGOT/chapters/Bran_1_1.txt')\n",
    "entity_dict = nltk_NER(book)\n",
    "unique_names = get_names_from_NER(entity_dict)\n",
    "\n",
    "for (name, num) in unique_names:\n",
    "    if True: #num > 1:\n",
    "        print(name, num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}